{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import lm_eval\n",
    "from lm_eval.models.huggingface import HFLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:3')\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779d459f966b44ed9b5558d5ac1bd663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'microsoft/phi-2' \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divergence Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sm_wassertain(logits1, logits2):\n",
    "    prob1 = F.softmax(logits1, dim=-1)\n",
    "    prob2 = F.softmax(logits2, dim=-1)\n",
    "    \n",
    "    cdf1 = torch.cumsum(prob1, dim=-1)\n",
    "    cdf2 = torch.cumsum(prob2, dim=-1)\n",
    "    \n",
    "    wasserstein_dist = torch.sum(torch.abs(cdf1 - cdf2), dim=-1)\n",
    "    \n",
    "    return wasserstein_dist.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_div(p, q, epsilon=1e-10):\n",
    "    p = p + epsilon\n",
    "    q = q + epsilon\n",
    "    return (p * (torch.log2(p) - torch.log2(q))).sum()\n",
    "\n",
    "def sm_jsd(p, q):\n",
    "    \"\"\"Returns the Jensen-Shannon Divergence of softmax-ed logits\"\"\"\n",
    "    p = F.softmax(p, dim=-1)\n",
    "    q = F.softmax(q, dim=-1)\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * kl_div(p, m) + 0.5 * kl_div(q, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29:03:09:38,404 WARNING  [huggingface.py:118] `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "2024-05-29:03:09:38,424 WARNING  [huggingface.py:337] Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "2024-05-29:03:09:38,426 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-05-29:03:09:48,544 INFO     [evaluator.py:218] num_fewshot has been set to 0 for truthfulqa_mc1 in its config. Manual configuration will be ignored.\n",
      "2024-05-29:03:09:48,545 INFO     [evaluator.py:218] num_fewshot has been set to 0 for truthfulqa_mc2 in its config. Manual configuration will be ignored.\n",
      "2024-05-29:03:09:48,545 INFO     [evaluator.py:218] num_fewshot has been set to 0 for truthfulqa_gen in its config. Manual configuration will be ignored.\n",
      "2024-05-29:03:09:48,549 INFO     [task.py:395] Building contexts for truthfulqa_mc1 on rank 0...\n",
      "100%|██████████| 10/10 [00:00<00:00, 681.89it/s]\n",
      "2024-05-29:03:09:48,567 INFO     [task.py:395] Building contexts for truthfulqa_mc2 on rank 0...\n",
      "100%|██████████| 10/10 [00:00<00:00, 1055.25it/s]\n",
      "2024-05-29:03:09:48,579 INFO     [task.py:395] Building contexts for truthfulqa_gen on rank 0...\n",
      "100%|██████████| 10/10 [00:00<00:00, 1727.69it/s]\n",
      "2024-05-29:03:09:48,589 INFO     [evaluator.py:362] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|██████████| 140/140 [00:04<00:00, 30.86it/s]\n",
      "2024-05-29:03:09:53,198 INFO     [evaluator.py:362] Running generate_until requests\n",
      "Running generate_until requests: 100%|██████████| 10/10 [00:04<00:00,  2.15it/s]\n",
      "2024-05-29:03:09:57,855 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:09:57,923 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:09:57,991 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:09:58,061 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:09:58,129 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:09:58,197 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:09:58,265 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:09:58,334 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:09:58,402 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:09:58,470 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:09:58,539 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:09:58,607 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:09:58,675 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:09:58,746 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:09:58,814 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:09:58,882 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:09:58,952 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:09:59,021 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:09:59,096 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:09:59,172 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:09:59,245 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:09:59,322 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:09:59,398 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:09:59,471 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:09:59,548 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:09:59,625 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:09:59,699 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:09:59,772 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:09:59,848 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:09:59,921 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:09:59,993 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:00,069 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:00,145 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:00,215 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:00,283 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:00,357 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:00,430 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:00,505 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:00,576 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:00,651 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:00,727 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:00,799 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:00,877 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:00,953 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:01,026 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:01,102 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:01,179 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:01,253 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:01,326 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:01,402 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:01,476 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:01,549 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:01,625 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:01,701 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:01,774 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:01,851 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:01,926 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:02,000 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:02,072 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:02,147 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:02,222 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:02,294 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:02,368 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:02,438 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:02,511 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:02,586 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:02,663 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:02,736 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:02,810 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:02,886 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:02,961 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:03,036 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:03,112 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:03,186 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:03,257 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:03,333 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:03,407 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:03,479 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:03,555 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:03,632 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:03,709 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:03,782 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:03,855 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:03,927 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:03,999 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:04,067 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:04,142 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:04,217 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:04,293 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:04,365 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:04,442 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:04,518 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-29:03:10:04,589 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "fatal: not a git repository (or any of the parent directories): .git\n"
     ]
    }
   ],
   "source": [
    "lm_obj = HFLM(pretrained=model, tokenizer=tokenizer)\n",
    "results = lm_eval.simple_evaluate(\n",
    "    model=lm_obj,\n",
    "    tasks=[\"truthfulqa\"],\n",
    "    num_fewshot=0,\n",
    "    limit=10,\n",
    "    log_samples=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1001758/3071663309.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  df['Baseline Accuracy'] = df[['acc,none','acc_stderr,none']].apply(lambda x : '{} ± {}'.format(round(x[0],2), round(x[1], 4)), axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Baseline Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>truthfulqa</th>\n",
       "      <td>0.51 ± 0.1097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>truthfulqa_gen</th>\n",
       "      <td>nan ± nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>truthfulqa_mc1</th>\n",
       "      <td>0.4 ± 0.1633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>truthfulqa_mc2</th>\n",
       "      <td>0.61 ± 0.1465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Baseline Accuracy\n",
       "truthfulqa         0.51 ± 0.1097\n",
       "truthfulqa_gen         nan ± nan\n",
       "truthfulqa_mc1      0.4 ± 0.1633\n",
       "truthfulqa_mc2     0.61 ± 0.1465"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results['results']).transpose()\n",
    "df['Baseline Accuracy'] = df[['acc,none','acc_stderr,none']].apply(lambda x : '{} ± {}'.format(round(x[0],2), round(x[1], 4)), axis=1)\n",
    "df[['Baseline Accuracy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# def calculate_max_length(questions, answer_choices, tokenizer):\n",
    "#     max_length = 0\n",
    "#     for question, choices in zip(questions, answer_choices):\n",
    "#         for choice in choices:\n",
    "#             combined_length = len(tokenizer.encode(question + \" \" + choice))\n",
    "#             if combined_length > max_length:\n",
    "#                 max_length = combined_length\n",
    "#     return max_length\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, tokenizer, max_length):\n",
    "        self.dataset = load_dataset(\"truthful_qa\", \"multiple_choice\", split=\"validation\")\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.dataset[idx]['question']\n",
    "        correct_choice_idx = self.dataset[idx]['mc1_targets']['labels'].index(1)\n",
    "        answer = self.dataset[idx]['mc1_targets']['choices'][correct_choice_idx]\n",
    "\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            f\"Instruct: {question}\\nOutput:{answer}\",\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "       \n",
    "        return {\n",
    "            'input_ids': encoding[\"input_ids\"].squeeze(),\n",
    "            'attention_mask': encoding[\"attention_mask\"].squeeze(),\n",
    "            'labels': encoding[\"input_ids\"].squeeze(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union, Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn.parameter import Parameter\n",
    "from transformers.models.phi.modeling_phi import PhiForCausalLM\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "class PhiDOLa(PhiForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.final_layernorm = torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        # self.layer_weights = Parameter(torch.cat([torch.zeros(config.num_hidden_layers-1), torch.ones(1)]))\n",
    "        self.layer_combnet = Parameter(32,1)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids: torch.LongTensor = None,\n",
    "            attention_mask: Optional[torch.Tensor] = None,\n",
    "            position_ids: Optional[torch.LongTensor] = None,\n",
    "            past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "            inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "            labels: Optional[torch.LongTensor] = None,\n",
    "            use_cache: Optional[bool] = None,\n",
    "            output_attentions: Optional[bool] = None,\n",
    "            output_hidden_states: Optional[bool] = None,\n",
    "            return_dict: Optional[bool] = None,\n",
    "        ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "\n",
    "            output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "            output_hidden_states = True\n",
    "            return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_values=past_key_values,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                use_cache=use_cache,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "\n",
    "            hidden_states = outputs[0]\n",
    "            logits = hidden_states \n",
    "            logits = self.lm_head(hidden_states)\n",
    "            logits = logits.float()\n",
    "\n",
    "            logits_list = []\n",
    "            \n",
    "            for hidden_state in outputs.hidden_states[:-1]:\n",
    "                int_logits = self.lm_head(self.final_layernorm(hidden_state)).float()\n",
    "                logits_list.append(int_logits.unsqueeze(-1))\n",
    "\n",
    "            logits_tensor = torch.cat(logits_list, dim=-1)\n",
    "            # print(logits_tensor.shape)\n",
    "\n",
    "            # weighted_logits = torch.zeros_like(logits_list[0])\n",
    "            # for i, logits_ in enumerate(logits_list):\n",
    "            #     weighted_logits += self.layer_weights[i] * logits_\n",
    "\n",
    "            # logits = weighted_logits\n",
    "\n",
    "            logits = self.layer_combnet(logits_tensor).squeeze(-1)\n",
    "            print(logits.shape)\n",
    "\n",
    "            loss = None\n",
    "            if labels is not None:\n",
    "                # Shift so that tokens < n predict n\n",
    "                shift_logits = logits[..., :-1, :].contiguous()\n",
    "                shift_labels = labels[..., 1:].contiguous()\n",
    "                # Flatten the tokens\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "                shift_labels = shift_labels.view(-1)\n",
    "                # Enable model parallelism\n",
    "                shift_labels = shift_labels.to(shift_logits.device)\n",
    "                loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "            if not return_dict:\n",
    "                output = (logits,) + outputs[1:]\n",
    "                return (loss,) + output if loss is not None else output\n",
    "\n",
    "            return CausalLMOutputWithPast(\n",
    "                loss=loss,\n",
    "                logits=logits,\n",
    "                past_key_values=outputs.past_key_values,\n",
    "                hidden_states=outputs.hidden_states,\n",
    "                attentions=outputs.attentions,\n",
    "            )\n",
    "\n",
    "def custom_loss_fn(log_probs, correct_choice_idx):\n",
    "    correct_log_prob = log_probs[correct_choice_idx]\n",
    "    loss = -correct_log_prob.mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def freeze_model_except_layer_weights(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.layer_combnet.requires_grad = True\n",
    "\n",
    "\n",
    "def fine_tune_layer_weights(model, dataloader, optimizer, num_epochs, tokenizer):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for data in dataloader:\n",
    "            input_ids = data['input_ids']\n",
    "            attention_mask = data['attention_mask']\n",
    "            labels = data['labels']\n",
    "\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            print(outputs.loss)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3e94523f314712837bb5ed9da46804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PhiDOLa were not initialized from the model checkpoint at microsoft/phi-2 and are newly initialized: ['final_layernorm.bias', 'final_layernorm.weight', 'layer_combnet.bias', 'layer_combnet.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 64, 51200])\n",
      "tensor(12.2787, device='cuda:3')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m freeze_model_except_layer_weights(model)\n\u001b[1;32m      9\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW([\u001b[38;5;241m*\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayer_combnet\u001b[38;5;241m.\u001b[39mparameters()], lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mfine_tune_layer_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 122\u001b[0m, in \u001b[0;36mfine_tune_layer_weights\u001b[0;34m(model, dataloader, optimizer, num_epochs, tokenizer)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mloss)\n\u001b[1;32m    121\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m--> 122\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:513\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \n\u001b[1;32m    468\u001b[0m \u001b[38;5;124;03mThe graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;124;03m        used to compute the attr::tensors.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/overrides.py:1604\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[1;32m   1602\u001b[0m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[1;32m   1603\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[0;32m-> 1604\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mmode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1605\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1606\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "\n",
    "             \n",
    "dataset = QADataset(tokenizer, max_length=64)\n",
    "train_split, test_split = torch.utils.data.random_split(dataset, [0.8, 0.2], generator=torch.Generator(device=device))\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=8)\n",
    "test_loader = DataLoader(dataset, batch_size=8)\n",
    "\n",
    "model = PhiDOLa.from_pretrained(model_name)\n",
    "freeze_model_except_layer_weights(model)\n",
    "optimizer = torch.optim.AdamW([*model.layer_combnet.parameters()], lr=1e-3)\n",
    "\n",
    "fine_tune_layer_weights(model, train_loader, optimizer, num_epochs=50, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-4.0275e-02, -2.9973e-02, -1.6777e-02, -2.8931e-02, -3.0201e-02,\n",
       "        -2.9434e-02, -2.8702e-02, -2.7871e-02, -2.7116e-02, -2.6579e-02,\n",
       "        -2.4894e-02, -2.4658e-02, -2.4114e-02, -2.3455e-02, -2.2129e-02,\n",
       "        -2.0788e-02, -1.9976e-02, -1.8693e-02, -1.6270e-02, -1.2697e-02,\n",
       "        -9.8543e-03, -8.1205e-03, -5.9172e-03, -3.3296e-03, -9.3828e-04,\n",
       "         1.8110e-03, -2.5356e-03, -7.1096e-04, -1.4312e-03, -3.8122e-03,\n",
       "        -1.2069e-02,  9.8400e-01], device='cuda:3', requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layer_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28:19:57:26,580 WARNING  [huggingface.py:118] `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "2024-05-28:19:57:26,602 WARNING  [huggingface.py:337] Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "2024-05-28:19:57:26,606 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-05-28:19:57:35,451 INFO     [evaluator.py:218] num_fewshot has been set to 0 for truthfulqa_mc1 in its config. Manual configuration will be ignored.\n",
      "2024-05-28:19:57:35,452 INFO     [evaluator.py:218] num_fewshot has been set to 0 for truthfulqa_mc2 in its config. Manual configuration will be ignored.\n",
      "2024-05-28:19:57:35,453 INFO     [evaluator.py:218] num_fewshot has been set to 0 for truthfulqa_gen in its config. Manual configuration will be ignored.\n",
      "2024-05-28:19:57:35,455 INFO     [task.py:395] Building contexts for truthfulqa_mc1 on rank 0...\n",
      "100%|██████████| 10/10 [00:00<00:00, 1049.86it/s]\n",
      "2024-05-28:19:57:35,469 INFO     [task.py:395] Building contexts for truthfulqa_mc2 on rank 0...\n",
      "100%|██████████| 10/10 [00:00<00:00, 1052.34it/s]\n",
      "2024-05-28:19:57:35,482 INFO     [task.py:395] Building contexts for truthfulqa_gen on rank 0...\n",
      "100%|██████████| 10/10 [00:00<00:00, 1835.58it/s]\n",
      "2024-05-28:19:57:35,493 INFO     [evaluator.py:362] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|██████████| 140/140 [00:11<00:00, 12.45it/s]\n",
      "2024-05-28:19:57:46,810 INFO     [evaluator.py:362] Running generate_until requests\n",
      "Running generate_until requests: 100%|██████████| 10/10 [00:44<00:00,  4.42s/it]\n",
      "2024-05-28:19:58:31,061 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:31,136 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:31,215 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:31,294 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:31,373 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:31,442 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:31,521 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:31,599 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:31,669 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:31,747 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:31,825 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:31,900 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:31,974 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:32,049 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:32,127 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:32,202 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:32,277 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:32,352 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:32,427 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:32,503 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:32,577 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:32,656 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:32,731 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:32,815 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:32,886 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:32,957 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:33,033 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:33,111 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:33,189 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:33,271 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:33,347 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:33,422 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:33,495 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:33,575 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:33,651 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:33,721 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:33,798 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:33,876 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:33,952 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:34,029 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:34,112 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:34,186 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:34,267 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:34,341 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:34,415 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:34,492 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:34,566 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:34,641 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:34,718 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:34,792 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:34,865 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:34,943 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:35,017 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:35,091 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:35,173 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:35,249 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:35,325 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:35,404 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:35,479 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:35,555 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:35,640 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:35,718 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:35,794 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:35,869 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:35,943 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:36,017 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:36,091 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:36,170 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:36,244 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:36,319 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:36,398 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:36,477 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:36,552 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:36,627 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:36,701 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:36,779 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:36,853 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:36,928 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:37,006 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:37,082 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:37,157 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:37,231 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:37,310 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:37,388 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:37,459 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:37,543 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:37,621 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:37,698 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:37,777 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:37,848 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:37,927 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:38,006 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "2024-05-28:19:58:38,086 INFO     [rouge_scorer.py:83] Using default tokenizer.\n",
      "fatal: not a git repository (or any of the parent directories): .git\n"
     ]
    }
   ],
   "source": [
    "lm_obj = HFLM(pretrained=model, tokenizer=tokenizer)\n",
    "results = lm_eval.simple_evaluate(\n",
    "    model=lm_obj,\n",
    "    tasks=[\"truthfulqa\"],\n",
    "    num_fewshot=0,\n",
    "    limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_843817/1554511284.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  df['DOLa Accuracy'] = df_dola[['acc,none','acc_stderr,none']].apply(lambda x : '{} ± {}'.format(round(x[0],2), round(x[1], 4)), axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Baseline Accuracy</th>\n",
       "      <th>DOLa Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>truthfulqa</th>\n",
       "      <td>0.51 ± 0.1097</td>\n",
       "      <td>0.4 ± 0.1076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>truthfulqa_gen</th>\n",
       "      <td>nan ± nan</td>\n",
       "      <td>nan ± nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>truthfulqa_mc1</th>\n",
       "      <td>0.4 ± 0.1633</td>\n",
       "      <td>0.4 ± 0.1633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>truthfulqa_mc2</th>\n",
       "      <td>0.61 ± 0.1465</td>\n",
       "      <td>0.4 ± 0.1402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Baseline Accuracy DOLa Accuracy\n",
       "truthfulqa         0.51 ± 0.1097  0.4 ± 0.1076\n",
       "truthfulqa_gen         nan ± nan     nan ± nan\n",
       "truthfulqa_mc1      0.4 ± 0.1633  0.4 ± 0.1633\n",
       "truthfulqa_mc2     0.61 ± 0.1465  0.4 ± 0.1402"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dola = pd.DataFrame(results['results']).transpose()\n",
    "df['DOLa Accuracy'] = df_dola[['acc,none','acc_stderr,none']].apply(lambda x : '{} ± {}'.format(round(x[0],2), round(x[1], 4)), axis=1)\n",
    "df[['Baseline Accuracy','DOLa Accuracy' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
